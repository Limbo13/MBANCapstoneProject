{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43723d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "#Some of these aren't used, but got included here just in case (would need testing to remove unnecessary modules)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from dmba import plotDecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import max_error,mean_absolute_error,mean_squared_log_error, mean_squared_error, r2_score\n",
    "from flaml import AutoML\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "from flaml.data import get_output_from_log\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30492207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV\n",
    "full_df = pd.read_csv(r'<path to file>\\Full_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping less columns than before\n",
    "full_df.drop(columns={'blurb','country','name','CreatedDate','DeadLineDate','LaunchedDate','id','CreatorID','backers_count','pledged'},axis=1,inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run this next set of codeblocks once\n",
    "#Changing goal and pledged amounts to account for inflation\n",
    "dictInflation = {2008:1.268,2009:1.273,2010:1.252,2011:1.214,2012:1.189,2013:1.172,2014:1.153,2015:1.152,2016:1.137,2017:1.114,2018:1.087,2019:1.068,2020:1.055,2021:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1cab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an inflation adjusted column\n",
    "full_df['usd_pledged_21'] = 0\n",
    "for index, row in full_df.iterrows():\n",
    "    print(index)\n",
    "    full_df.loc[index,'usd_pledged_21'] = round(dictInflation[row['DeadLineDate_Year']]*row['usd_pledged'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the original pledged column\n",
    "full_df.drop(columns={'usd_pledged'},axis=1,inplace=True,errors='ignore')\n",
    "full_df['usd_pledged_21'] = full_df['usd_pledged_21'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank the projects in each category/subcategory\n",
    "#This will help with categorization\n",
    "new_df = pd.DataFrame()\n",
    "for name, group in full_df.groupby(['CatID', 'SubCatID']):\n",
    "    part_df = full_df[(full_df.CatID == name[0])&(full_df.SubCatID==name[1])]\n",
    "    percRank = part_df.usd_pledged_21.rank(pct=True)\n",
    "    part_df['PercentileRank'] = percRank\n",
    "    \n",
    "    print(name)\n",
    "    print(len(part_df))\n",
    "    \n",
    "    for index,row in part_df.iterrows():\n",
    "        rankVal = row['PercentileRank']\n",
    "        if rankVal <= 0.1:\n",
    "            part_df.loc[index,'Tens'] = 10\n",
    "        elif rankVal <= 0.2:\n",
    "            part_df.loc[index,'Tens'] = 20\n",
    "        elif rankVal <= 0.3:\n",
    "            part_df.loc[index,'Tens'] = 30\n",
    "        elif rankVal <= 0.4:\n",
    "            part_df.loc[index,'Tens'] = 40\n",
    "        elif rankVal <= 0.5:\n",
    "            part_df.loc[index,'Tens'] = 50\n",
    "        elif rankVal <= 0.6:\n",
    "            part_df.loc[index,'Tens'] = 60\n",
    "        elif rankVal <= 0.7:\n",
    "            part_df.loc[index,'Tens'] = 70\n",
    "        elif rankVal <= 0.8:\n",
    "            part_df.loc[index,'Tens'] = 80\n",
    "        elif rankVal <= 0.9:\n",
    "            part_df.loc[index,'Tens'] = 90\n",
    "        else:\n",
    "            part_df.loc[index,'Tens'] = 100\n",
    "\n",
    "        if rankVal <= 0.2:\n",
    "            part_df.loc[index,'Twenties'] = 20\n",
    "        elif rankVal <= 0.4:\n",
    "            part_df.loc[index,'Twenties'] = 40\n",
    "        elif rankVal <= 0.6:\n",
    "            part_df.loc[index,'Twenties'] = 60\n",
    "        elif rankVal <= 0.8:\n",
    "            part_df.loc[index,'Twenties'] = 80\n",
    "        else:\n",
    "            part_df.loc[index,'Twenties'] = 100\n",
    "\n",
    "        if rankVal <= 0.33:\n",
    "            part_df.loc[index,'Third'] = 33\n",
    "        elif rankVal <= 0.66:\n",
    "            part_df.loc[index,'Third'] = 66\n",
    "        else:\n",
    "            part_df.loc[index,'Third'] = 100\n",
    "\n",
    "        if rankVal <= 0.5:\n",
    "            part_df.loc[index,'Half'] = 50\n",
    "        else:\n",
    "            part_df.loc[index,'Half'] = 100\n",
    "            \n",
    "    new_df = new_df.append(part_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert rank to brackets\n",
    "part_df = full_df[full_df['CatID'].isnull()]\n",
    "percRank = part_df.usd_pledged_21.rank(pct=True)\n",
    "part_df['PercentileRank'] = percRank\n",
    "\n",
    "for index,row in part_df.iterrows():\n",
    "    rankVal = row['PercentileRank']\n",
    "    if rankVal <= 0.1:\n",
    "        part_df.loc[index,'Tens'] = 10\n",
    "    elif rankVal <= 0.2:\n",
    "        part_df.loc[index,'Tens'] = 20\n",
    "    elif rankVal <= 0.3:\n",
    "        part_df.loc[index,'Tens'] = 30\n",
    "    elif rankVal <= 0.4:\n",
    "        part_df.loc[index,'Tens'] = 40\n",
    "    elif rankVal <= 0.5:\n",
    "        part_df.loc[index,'Tens'] = 50\n",
    "    elif rankVal <= 0.6:\n",
    "        part_df.loc[index,'Tens'] = 60\n",
    "    elif rankVal <= 0.7:\n",
    "        part_df.loc[index,'Tens'] = 70\n",
    "    elif rankVal <= 0.8:\n",
    "        part_df.loc[index,'Tens'] = 80\n",
    "    elif rankVal <= 0.9:\n",
    "        part_df.loc[index,'Tens'] = 90\n",
    "    else:\n",
    "        part_df.loc[index,'Tens'] = 100\n",
    "\n",
    "    if rankVal <= 0.2:\n",
    "        part_df.loc[index,'Twenties'] = 20\n",
    "    elif rankVal <= 0.4:\n",
    "        part_df.loc[index,'Twenties'] = 40\n",
    "    elif rankVal <= 0.6:\n",
    "        part_df.loc[index,'Twenties'] = 60\n",
    "    elif rankVal <= 0.8:\n",
    "        part_df.loc[index,'Twenties'] = 80\n",
    "    else:\n",
    "        part_df.loc[index,'Twenties'] = 100\n",
    "\n",
    "    if rankVal <= 0.33:\n",
    "        part_df.loc[index,'Third'] = 33\n",
    "    elif rankVal <= 0.66:\n",
    "        part_df.loc[index,'Third'] = 66\n",
    "    else:\n",
    "        part_df.loc[index,'Third'] = 100\n",
    "\n",
    "    if rankVal <= 0.5:\n",
    "        part_df.loc[index,'Half'] = 50\n",
    "    else:\n",
    "        part_df.loc[index,'Half'] = 100\n",
    "\n",
    "new_df = new_df.append(part_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start creating the corpus\n",
    "Corpus = new_df\n",
    "Corpus.blurb = Corpus.blurb.astype(str)\n",
    "Corpus.name = Corpus.name.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - a : Remove blank rows if any.\n",
    "Corpus['name'].dropna(inplace=True)\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['name'] = [entry.lower() for entry in Corpus['name']]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['name']= [word_tokenize(entry) for entry in Corpus['name']]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['name']):\n",
    "    print(index)\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus.loc[index,'text_final_name'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f71641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "Corpus.to_csv(r'<path to file>\\MainDataset\\Full_Corpus.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
